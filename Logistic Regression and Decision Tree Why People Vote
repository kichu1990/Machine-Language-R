Vote=read.csv("C:\\Users\\ABHIC6\\Downloads\\gerber.csv")
View(Vote)

#What proportion of people in this dataset voted in this election?
prop.table(table(Vote$voting))

#Which of the four "treatment groups" had the largest percentage of people who actually voted (voting = 1)?
tapply(Vote$voting,Vote$civicduty,mean)
tapply(Vote$voting,Vote$hawthorne,mean)
tapply(Vote$voting,Vote$self,mean)
tapply(Vote$voting,Vote$neighbors,mean)

#Build a logistic regression model for voting using the four treatment group variables as the independent variables
Model_1=glm(voting~civicduty+hawthorne+self+neighbors,data=Vote,family = binomial)
summary(Model_1)

#Using a threshold of 0.3, what is the accuracy of the logistic regression model? 
predictLog=predict(Model_1,type="response")
table(Vote$voting, predictLog > 0.3)

#accuracy
(134513+51966)/(134513+51966+100875+56730)

#Using a threshold of 0.5, what is the accuracy of the logistic regression model? 
predictLog=predict(Model_1,type="response")
table(Vote$voting, predictLog > 0.5)

#accuracy
(235388+0)/(235388+108696)

library(ROCR)
library(rpart)
library(rpart.plot)
ROCRpred = prediction(predictLog, Vote$voting)
as.numeric(performance(ROCRpred, "auc")@y.values)

CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=Vote)
prp(CARTmodel)

CARTmodel2 = rpart(voting ~civicduty+hawthorne+self+neighbors,data=Vote,cp=0.0)
prp(CARTmodel2)

#Using only the CART tree plot, determine what fraction (a number between 0 and 1) of 
#"Civic Duty" people voted:0.31

#Make a new tree that includes the "sex" variable, again with cp = 0.0
CARTmodel3 = rpart(voting ~civicduty+hawthorne+self+neighbors+sex+control,data=Vote,cp=0.0)
prp(CARTmodel3)

#Create a regression tree using just the "control" variable, then create another tree with the "control" and "sex" variables, both with cp=0.0.
CARTcontrol = rpart(voting ~ control, data=Vote, cp=0.0)
CARTsex = rpart(voting ~ control + sex, data=Vote, cp=0.0)

prp(CARTcontrol,digits=6)
abs(0.296638-0.34)
prp(CARTsex,digits=6)

#Now, using the second tree (with control and sex), determine who is affected more by NOT being in the control group
#They are affected about the same (change in probability within 0.001 of each other).

#Going back to logistic regression now, create a model using "sex" and "control". Interpret the coefficient for "sex":
logModel=glm(voting~ control + sex,data=Vote,family = binomial)
summary(logModel)
#Coefficient is negative, reflecting that women are less likely to vote.

#The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control),
#(Woman, Not Control), (Woman, Control). Logistic regression has attempted to do the same, although it wasn't able to do as well because
#it can't consider exactly the joint possibility of being a women and in the control group.

#We can quantify this precisely. Create the following dataframe (this contains all of the possible values of sex and control), and 
#evaluate your logistic regression using the predict function (where "LogModelSex" is the name of your logistic regression model that 
#uses both control and sex):
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(logModel, newdata=Possibilities, type="response")

#What is the absolute difference between the tree and the logistic regression for the (Woman, Control) case? 
abs(.290456-0.2908065)

#We're going to add a new term to our logistic regression now, that is the combination of the "sex" and "control" variables - so 
#if this new variable is 1, that means the person is a woman AND in the control group
logModel2 = glm(voting ~ sex + control + sex:control, data=Vote, family="binomial")
summary(logModel2)
#This coefficient is negative, so that means that a value of 1 in this variable decreases the chance of voting. 
#This variable will have variable 1 if the person is a woman and in the control group.

predict(logModel2, newdata=Possibilities, type="response")

#what is the difference between the logistic regression model and the CART model for the (Woman, Control) case? 
#Again, give your answer with five numbers after the decimal point.
abs(0.2904558-0.290456)

#trees can capture nonlinear relationships that logistic regression can not, but that we can get around this sometimes by using 
#variables that are the combination of two variables.
#We should not use all possible interaction terms in a logistic regression model due to overfitting. Even in this simple problem, 
#we have four treatment groups and two values for sex. If we have an interaction term for every treatment variable with sex, 
#we will double the number of variables. In smaller data sets, this could quickly lead to overfitting.
